import os
from pydub import AudioSegment
import subprocess
from .config import get_emotion_path
from .gptapi import analyze_emotion

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
CHORD_DIR = os.path.join(BASE_DIR, "chord_note_test")
TOTAL_DURATION_SEC = 16
TOTAL_BEATS = 32
MIN_BEATS_PER_CHORD = 4
MAX_BEATS_PER_CHORD = 6

# æ™‚å€¼åº«ï¼šBPM = 120ï¼Œæ¯æ‹ 0.5 ç§’
note_durations = {
    "4N": 0.5,    #0
    "4.N": 0.75,     #1
    "8.N": 0.375,    #2 å–æ¶ˆ8åˆ†éŸ³ç¬¦æ”¹8åˆ†é™„é»
    "4R": 0.5,     #3
    "2.N": 1.5,    #4
    "16N": 1.25,   #5 0.125æ”¹æˆ1.25
    "8.NN": 1.75,  #6 æ”¹1.75
    "2R": 1,     #7 äºŒåˆ†ä¼‘æ­¢ç¬¦
    "2N": 1.0,     #8 å››åˆ†ä¼‘æ­¢ç¬¦
    "8R": 0.5,    #9 å…«åˆ†ä¼‘æ­¢ç¬¦æ”¹å››åˆ†ä¼‘æ­¢
}

NOTE_KEYS = list(note_durations.keys())

def get_rhythm_pattern(weights, melody_indices):
    """æ ¹æ“šåŠ æ¬Šèˆ‡ä¸»æ—‹å¾‹ index ç”Ÿæˆç¯€å¥é•·åº¦åºåˆ—"""
    W = weights[:8]
    A = [(w % 10) + 1 for w in W]
    B = [((w % 7) + 1) if (w % 7) != 9 else 9 for w in W]
    products = [a * b for a, b in zip(A, B)]
    pattern_seed = ''.join(str(p) for p in products) + ''.join(str(i) for i in melody_indices)

    print(products)
    print(pattern_seed)
    return pattern_seed

def assign_rhythm_lengths(pattern_seed):
    """æ ¹æ“šå®Œæ•´ç¯€å¥ç¨®å­ï¼Œç”¢ç”Ÿç¯€å¥é•·åº¦åºåˆ—ï¼ˆåŒ…å«ä¼‘æ­¢ç¬¦ï¼‰"""
    rhythm_lengths = []
    for digit in pattern_seed:
        digit = int(digit)
        key_index = digit % len(NOTE_KEYS)
        rhythm_key = NOTE_KEYS[key_index]
        rhythm_lengths.append(rhythm_key)
    print(rhythm_lengths)    
    return rhythm_lengths

# ä¸»æ—‹å¾‹éŸ³æª”
def combine_melody_audio(melody_notes, rhythm_lengths, sentence: str):
    
    beat_ms = (TOTAL_DURATION_SEC * 1000) / TOTAL_BEATS
    output = AudioSegment.silent(duration=0)
    melody_ptr = 0
    current_duration = 0

    #emotion = os.path.basename(emotion)
    emotion = analyze_emotion(sentence)
    print(emotion)
    
    for rhythm_key in rhythm_lengths:
        segment_ms = int(note_durations[rhythm_key] * beat_ms *2)
        # âœ… è‹¥æƒ…ç·’æ˜¯æ‚²å‚·â†’å»¶é•·éŸ³ç¬¦ 1.5 å€
        if emotion == "SADNESS":
            segment_ms = segment_ms * 1.5

        segment_ms = int(segment_ms)
        
        # â›” è‹¥åŠ ä¸Šé€™æ®µæœƒè¶…é 16 ç§’ï¼Œå°±çµæŸ
        if current_duration + segment_ms > TOTAL_DURATION_SEC * 1000:
            break
        
        rhythm_index = NOTE_KEYS.index(rhythm_key)

        # ğŸµ 7,8,9 -> ä½¿ç”¨ä¼‘æ­¢ç¬¦ï¼Œä¸æ¶ˆè€— melody_ptr
        if rhythm_index in [7, 3, 9]:
            segment = AudioSegment.silent(duration=segment_ms)
        #if rhythm_key.endswith("R"):  # ä¼‘æ­¢ç¬¦è™•ç†
        #    rest_path = os.path.join(AUDIO_DIR, f"{rhythm_key}.wav")
        #    if os.path.exists(rest_path):
        #        segment = AudioSegment.from_wav(rest_path)
        #        segment = segment[:segment_ms]
        #    else:
        #        st.warning(f"âš ï¸ æ‰¾ä¸åˆ°ä¼‘æ­¢ç¬¦éŸ³æª”ï¼š{rest_path}")
        #        segment = AudioSegment.silent(duration=segment_ms)
        else:  # éŸ³ç¬¦è™•ç†
            if melody_ptr >= len(melody_notes):
                break

            note_name = melody_notes[melody_ptr]
            melody_ptr += 1

            emotion_dir = os.path.join(CHORD_DIR, emotion)
            note_path = os.path.join(emotion_dir, f"{note_name}.wav")

            print(note_name)

            if os.path.exists(note_path):
                segment = AudioSegment.from_wav(note_path)
                segment = segment[:segment_ms]
            else:
                segment = AudioSegment.silent(duration=segment_ms)
            
        if len(segment) > 20 and len(output) > 20:
            output = output.append(segment, crossfade=1)
        else:
            output += segment
            
        current_duration += segment_ms  # âœ… è¨˜å¾—åŠ ç¸½æ™‚é–“
    # âœ… æ–°å¢ï¼šå¦‚æœä¸»æ—‹å¾‹ä¸æ»¿ 4 ç§’ï¼Œæ–¼ç¬¬ 4.5 ç§’é‡æ’­ä¸€æ¬¡
    if current_duration <= 8000:
        padding_before = AudioSegment.silent(duration=8500 - current_duration)
        output = output + padding_before + output
        
        output = output.fade_out(250)
    
    if emotion == "SADNESS": 
        output = output.apply_gain(+6)
    if emotion == "JOY": 
        output = output.apply_gain(+1)
    if emotion == "CALM": 
        output = output.apply_gain(+6)
    if emotion == "SURPRISE": 
        output = output.apply_gain(+5)
    print(current_duration)
    return output, current_duration

